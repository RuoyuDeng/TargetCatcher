{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'my': 2,\n",
       " 'love': 3,\n",
       " 'dog': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'cat': 7,\n",
       " 'do': 8,\n",
       " 'think': 9,\n",
       " 'is': 10,\n",
       " 'amazing': 11}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "             \"I love my dog\",\n",
    "             \"I love my cat\",\n",
    "             \"You love my dog!\",\n",
    "             \"Do you think my dog is amazing?\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    \"i really love my dog\",\n",
    "    'my dog loves my manatee'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 2],\n",
       "       [5, 3, 2],\n",
       "       [6, 3, 2],\n",
       "       [8, 6, 9]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"post\" truncating cutting from the back, \"pre\" truncating cutting from the front\n",
    "padded_seq = pad_sequences(sequences, padding=\"post\",truncating='post', maxlen=3)\n",
    "padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "test_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commonly used constant values (can be changed)\n",
    "\n",
    "vocab_size = 10000\n",
    "tr_size = 20000\n",
    "oov_tok = \"<OOV>\"\n",
    "padding_style = \"post\"\n",
    "\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_sarcastic': 1,\n",
       " 'headline': 'thirtysomething scientists unveil doomsday clock of hair loss',\n",
       " 'article_link': 'https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# obtain initial data\n",
    "datastore = []\n",
    "with open(r\"News-Headlines-Dataset-For-Sarcasm-Detection/Sarcasm_Headlines_Dataset.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        datastore.append(json.loads(line))\n",
    "\n",
    "datastore[0] # a list of json formated `dictionary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate lists of raw data we need\n",
    "labels = []\n",
    "sentences = []\n",
    "urls = []\n",
    "\n",
    "for item in datastore:\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    sentences.append(item['headline'])\n",
    "    urls.append(item[\"article_link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Raw data preprocessing (need to do it separately for training and test dataset, tokenization and generating sequences are essentially pre-processing data)\n",
    "\n",
    "# 1. Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 2. Sequence\n",
    "tr_sequence = tokenizer.texts_to_sequences(sentences)\n",
    "# 3. Padding -> output a np array (pad on post)\n",
    "padded = pad_sequences(tr_sequence, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28619, 152)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing pre-processed data for training (can be randomly slicing to ensure fairness)\n",
    "tr_sentences = sentences[0:tr_size]\n",
    "tr_labels = labels[0:tr_size]\n",
    "\n",
    "ts_sentences = sentences[tr_size:]\n",
    "ts_labels = labels[tr_size:]\n",
    "\n",
    "# Tokenizer can be shared for generating sequences for test and training data\n",
    "tokenizer = Tokenizer(oov_token=oov_tok, num_words=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25898"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we do not want the training data to have access to unseen words that have been tokenized initially, we need to redo our tokenization based on ONLY training sentences\n",
    "tokenizer.fit_on_texts(tr_sentences) # generate the tokens\n",
    "\n",
    "tr_seq = tokenizer.texts_to_sequences(tr_sentences) # change the sentences into matrix form of token representations of words\n",
    "\n",
    "tr_seq_padded = pad_sequences(tr_seq, padding=padding_style, maxlen=max_length, truncating=trunc_type) # pad the sequences to make sure every line has same length\n",
    "\n",
    "len(tokenizer.word_index.keys()) # less than the one above with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same procedure for test dataset as well\n",
    "tokenizer.fit_on_texts(ts_sentences)\n",
    "ts_seq = tokenizer.texts_to_sequences(ts_sentences)\n",
    "ts_seq_padded = pad_sequences(ts_seq, padding=padding_style, maxlen=max_length, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build Model (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding,GlobalAveragePooling1D,Dense\n",
    "\n",
    "# TODO: Continue Here\n",
    "model = Sequential(\n",
    "    [\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length), # embedding_dim = N refers to that each word in the vocabulary is transfered into a 1 x N list. word1 -> [0, 1, 2, .... N-1]\n",
    "        GlobalAveragePooling1D(), # [1,5,7] -> [3, 6] if padding='valid', pool_size=2, stride=1. output_dim = input_dim - pool_size + 1 / stride, we plus 1 to avoid error when pool_size = voca_size\n",
    "        Dense(32,activation='relu'),\n",
    "        Dense(1,activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                544       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,577\n",
      "Trainable params: 160,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.6412 - accuracy: 0.6345 - val_loss: 0.6254 - val_accuracy: 0.6370\n",
      "Epoch 2/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3823 - accuracy: 0.8392 - val_loss: 0.7262 - val_accuracy: 0.6320\n",
      "Epoch 3/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2920 - accuracy: 0.8834 - val_loss: 0.8209 - val_accuracy: 0.6199\n",
      "Epoch 4/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2432 - accuracy: 0.9047 - val_loss: 0.9173 - val_accuracy: 0.6095\n",
      "Epoch 5/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2097 - accuracy: 0.9194 - val_loss: 1.0039 - val_accuracy: 0.6085\n",
      "Epoch 6/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1843 - accuracy: 0.9298 - val_loss: 1.1240 - val_accuracy: 0.5968\n",
      "Epoch 7/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1619 - accuracy: 0.9402 - val_loss: 1.1842 - val_accuracy: 0.6036\n",
      "Epoch 8/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1441 - accuracy: 0.9475 - val_loss: 1.3160 - val_accuracy: 0.5938\n",
      "Epoch 9/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1324 - accuracy: 0.9520 - val_loss: 1.3821 - val_accuracy: 0.5930\n",
      "Epoch 10/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1184 - accuracy: 0.9575 - val_loss: 1.4468 - val_accuracy: 0.5933\n",
      "Epoch 11/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1073 - accuracy: 0.9629 - val_loss: 1.5370 - val_accuracy: 0.5915\n",
      "Epoch 12/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0985 - accuracy: 0.9669 - val_loss: 1.6428 - val_accuracy: 0.5881\n",
      "Epoch 13/30\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 0.0889 - accuracy: 0.9699 - val_loss: 1.7888 - val_accuracy: 0.5878\n",
      "Epoch 14/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0805 - accuracy: 0.9734 - val_loss: 1.8563 - val_accuracy: 0.5870\n",
      "Epoch 15/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0736 - accuracy: 0.9760 - val_loss: 1.9584 - val_accuracy: 0.5795\n",
      "Epoch 16/30\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 0.0648 - accuracy: 0.9797 - val_loss: 2.1959 - val_accuracy: 0.5820\n",
      "Epoch 17/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0602 - accuracy: 0.9808 - val_loss: 2.1840 - val_accuracy: 0.5809\n",
      "Epoch 18/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0566 - accuracy: 0.9819 - val_loss: 2.3053 - val_accuracy: 0.5798\n",
      "Epoch 19/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0542 - accuracy: 0.9829 - val_loss: 2.3894 - val_accuracy: 0.5769\n",
      "Epoch 20/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0478 - accuracy: 0.9857 - val_loss: 2.6652 - val_accuracy: 0.5752\n",
      "Epoch 21/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0429 - accuracy: 0.9871 - val_loss: 2.6228 - val_accuracy: 0.5763\n",
      "Epoch 22/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0387 - accuracy: 0.9895 - val_loss: 2.7702 - val_accuracy: 0.5763\n",
      "Epoch 23/30\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 0.0371 - accuracy: 0.9890 - val_loss: 2.8474 - val_accuracy: 0.5744\n",
      "Epoch 24/30\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 0.0345 - accuracy: 0.9891 - val_loss: 2.9519 - val_accuracy: 0.5721\n",
      "Epoch 25/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0312 - accuracy: 0.9913 - val_loss: 3.1534 - val_accuracy: 0.5751\n",
      "Epoch 26/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0283 - accuracy: 0.9920 - val_loss: 3.2118 - val_accuracy: 0.5725\n",
      "Epoch 27/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0260 - accuracy: 0.9924 - val_loss: 3.5267 - val_accuracy: 0.5722\n",
      "Epoch 28/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0258 - accuracy: 0.9917 - val_loss: 3.5502 - val_accuracy: 0.5749\n",
      "Epoch 29/30\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0204 - accuracy: 0.9946 - val_loss: 3.5671 - val_accuracy: 0.5675\n",
      "Epoch 30/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9952 - val_loss: 3.7789 - val_accuracy: 0.5718\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "# Note: labels and padded sequences MUST BE IN type of np.array, since we initially read the labels as list, we need to convert them into array here\n",
    "tr_labels = np.array(labels[0:tr_size])\n",
    "ts_labels = np.array(labels[tr_size:])\n",
    "\n",
    "history = model.fit(\n",
    "    tr_seq_padded,\n",
    "    tr_labels,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=(ts_seq_padded, ts_labels),\n",
    "    verbose=1 # define the GUI display of the training progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sentences = [\n",
    "    \"Today is so nice\",\n",
    "    \"You look like shit\",\n",
    "    \"they're really on top of things\"\n",
    "]\n",
    "\n",
    "tokenizer.fit_on_texts(ts_sentences)\n",
    "tmp_seq = tokenizer.texts_to_sequences(ts_sentences)\n",
    "tmp_padded = pad_sequences(tmp_seq, padding=padding_style, truncating=trunc_type, maxlen=max_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[735, 110, 8, 265, 3, 140]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_seq = tokenizer.texts_to_sequences([\"they're really on top of things\"])\n",
    "tmp_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1062523e-09],\n",
       "       [4.7942996e-04],\n",
       "       [1.1391908e-02]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tmp_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
