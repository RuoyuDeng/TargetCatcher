{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding,GlobalAveragePooling1D,Dense,Bidirectional,LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain initial data\n",
    "datastore = []\n",
    "with open(r\"News-Headlines-Dataset-For-Sarcasm-Detection/Sarcasm_Headlines_Dataset.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        datastore.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data and labels\n",
    "# generate lists of raw data we need (can also filter out all punctuation as part of preprocessing as well)\n",
    "labels = []\n",
    "sentences = []\n",
    "urls = []\n",
    "\n",
    "for item in datastore:\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    sentences.append(item['headline'])\n",
    "    urls.append(item[\"article_link\"])\n",
    "\n",
    "sentences = sentences[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for generating text, we only need the raw text data\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tr_seq = tokenizer.texts_to_sequences(sentences)\n",
    "unique_w = len(tokenizer.word_index) + 1 # len(tokenizer.word_index) \n",
    "# refer to the number of unique words, but since we also want to include 0 as the tokenization of a imaginary word, \n",
    "# we add 1 (we padded with 0, however in order to do prediction, we need to make sense of every token in the input sequence, thus we must put \"0\" as additional tokenizaed word in the dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "for line in tr_seq:\n",
    "    for i in range(1, len(line)):\n",
    "        raw_data.append(line[:i+1])\n",
    "# now pad these raw data, based on the largest sequence len\n",
    "max_seq_len = max([len(x) for x in raw_data])\n",
    "pad_raw_data = pad_sequences(raw_data, padding=\"pre\", maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = pad_raw_data[:,:-1]\n",
    "tr_label = to_categorical(pad_raw_data[:,-1], num_classes=unique_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_try = tr_set[:5000]\n",
    "label_try = tr_label[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27776, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(unique_w, input_length=max_seq_len-1, output_dim=240)) # max_seq_len - 1 because we drop 1 entry for label\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(unique_w, activation=\"softmax\"))\n",
    "\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 28, 240)           2172000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 300)              469200    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 9050)              2724050   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,365,250\n",
      "Trainable params: 5,365,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "868/868 [==============================] - 74s 81ms/step - loss: 8.7506 - accuracy: 0.0278\n",
      "Epoch 2/50\n",
      "868/868 [==============================] - 67s 78ms/step - loss: 10.0132 - accuracy: 0.0238\n",
      "Epoch 3/50\n",
      "868/868 [==============================] - 64s 74ms/step - loss: 9.3725 - accuracy: 0.0304\n",
      "Epoch 4/50\n",
      "868/868 [==============================] - 54s 62ms/step - loss: 8.7392 - accuracy: 0.0387\n",
      "Epoch 5/50\n",
      "868/868 [==============================] - 56s 65ms/step - loss: 7.4324 - accuracy: 0.0499\n",
      "Epoch 6/50\n",
      "868/868 [==============================] - 55s 63ms/step - loss: 6.7807 - accuracy: 0.0636\n",
      "Epoch 7/50\n",
      "868/868 [==============================] - 52s 59ms/step - loss: 6.2657 - accuracy: 0.0846\n",
      "Epoch 8/50\n",
      "868/868 [==============================] - 50s 57ms/step - loss: 5.6952 - accuracy: 0.1123\n",
      "Epoch 9/50\n",
      "868/868 [==============================] - 59s 68ms/step - loss: 5.2660 - accuracy: 0.1468\n",
      "Epoch 10/50\n",
      "868/868 [==============================] - 50s 58ms/step - loss: 4.8459 - accuracy: 0.1838\n",
      "Epoch 11/50\n",
      "868/868 [==============================] - 50s 58ms/step - loss: 4.6077 - accuracy: 0.2131\n",
      "Epoch 12/50\n",
      "868/868 [==============================] - 50s 58ms/step - loss: 4.3358 - accuracy: 0.2390\n",
      "Epoch 13/50\n",
      "868/868 [==============================] - 49s 57ms/step - loss: 4.1076 - accuracy: 0.2706\n",
      "Epoch 14/50\n",
      "868/868 [==============================] - 49s 56ms/step - loss: 3.9733 - accuracy: 0.2897\n",
      "Epoch 15/50\n",
      "868/868 [==============================] - 50s 57ms/step - loss: 3.9279 - accuracy: 0.2984\n",
      "Epoch 16/50\n",
      "868/868 [==============================] - 50s 57ms/step - loss: 3.7321 - accuracy: 0.3239\n",
      "Epoch 17/50\n",
      "868/868 [==============================] - 65s 75ms/step - loss: 3.5403 - accuracy: 0.3455\n",
      "Epoch 18/50\n",
      "868/868 [==============================] - 68s 79ms/step - loss: 3.4125 - accuracy: 0.3625\n",
      "Epoch 19/50\n",
      "868/868 [==============================] - 71s 82ms/step - loss: 3.3790 - accuracy: 0.3738\n",
      "Epoch 20/50\n",
      "868/868 [==============================] - 71s 81ms/step - loss: 3.3395 - accuracy: 0.3833\n",
      "Epoch 21/50\n",
      "868/868 [==============================] - 71s 82ms/step - loss: 3.3562 - accuracy: 0.3908\n",
      "Epoch 22/50\n",
      "868/868 [==============================] - 62s 71ms/step - loss: 3.2075 - accuracy: 0.4008\n",
      "Epoch 23/50\n",
      "868/868 [==============================] - 55s 63ms/step - loss: 3.1310 - accuracy: 0.4190\n",
      "Epoch 24/50\n",
      "868/868 [==============================] - 56s 64ms/step - loss: 3.0928 - accuracy: 0.4259\n",
      "Epoch 25/50\n",
      "868/868 [==============================] - 52s 59ms/step - loss: 3.0739 - accuracy: 0.42710s - loss: 3.0687 \n",
      "Epoch 26/50\n",
      "868/868 [==============================] - 53s 62ms/step - loss: 3.0208 - accuracy: 0.4352\n",
      "Epoch 27/50\n",
      "868/868 [==============================] - 51s 59ms/step - loss: 2.9346 - accuracy: 0.4534\n",
      "Epoch 28/50\n",
      "868/868 [==============================] - 49s 56ms/step - loss: 2.8609 - accuracy: 0.4614\n",
      "Epoch 29/50\n",
      "868/868 [==============================] - 49s 57ms/step - loss: 2.7977 - accuracy: 0.4697\n",
      "Epoch 30/50\n",
      "868/868 [==============================] - 49s 56ms/step - loss: 2.7714 - accuracy: 0.4767\n",
      "Epoch 31/50\n",
      "868/868 [==============================] - 51s 59ms/step - loss: 2.7558 - accuracy: 0.4864\n",
      "Epoch 32/50\n",
      "868/868 [==============================] - 50s 58ms/step - loss: 2.6975 - accuracy: 0.4922\n",
      "Epoch 33/50\n",
      "868/868 [==============================] - 51s 59ms/step - loss: 2.6524 - accuracy: 0.5012\n",
      "Epoch 34/50\n",
      "868/868 [==============================] - 50s 58ms/step - loss: 2.6007 - accuracy: 0.5054\n",
      "Epoch 35/50\n",
      "868/868 [==============================] - 49s 56ms/step - loss: 2.5377 - accuracy: 0.5207\n",
      "Epoch 36/50\n",
      "868/868 [==============================] - 49s 57ms/step - loss: 2.5196 - accuracy: 0.5242\n",
      "Epoch 37/50\n",
      "868/868 [==============================] - 52s 59ms/step - loss: 2.6910 - accuracy: 0.5059\n",
      "Epoch 38/50\n",
      "868/868 [==============================] - 49s 56ms/step - loss: 2.4847 - accuracy: 0.5248\n",
      "Epoch 39/50\n",
      "868/868 [==============================] - 51s 59ms/step - loss: 2.4230 - accuracy: 0.5344\n",
      "Epoch 40/50\n",
      "868/868 [==============================] - 55s 63ms/step - loss: 2.4307 - accuracy: 0.5359\n",
      "Epoch 41/50\n",
      "868/868 [==============================] - 52s 60ms/step - loss: 2.3674 - accuracy: 0.5478\n",
      "Epoch 42/50\n",
      "868/868 [==============================] - 50s 57ms/step - loss: 2.3230 - accuracy: 0.5491\n",
      "Epoch 43/50\n",
      "868/868 [==============================] - 52s 60ms/step - loss: 3.1410 - accuracy: 0.5298\n",
      "Epoch 44/50\n",
      "868/868 [==============================] - 51s 59ms/step - loss: 5.8386 - accuracy: 0.3182\n",
      "Epoch 45/50\n",
      "868/868 [==============================] - 48s 55ms/step - loss: 3.8984 - accuracy: 0.4217\n",
      "Epoch 46/50\n",
      "868/868 [==============================] - 51s 59ms/step - loss: 3.1557 - accuracy: 0.4827\n",
      "Epoch 47/50\n",
      "868/868 [==============================] - 49s 56ms/step - loss: 2.8129 - accuracy: 0.5135\n",
      "Epoch 48/50\n",
      "868/868 [==============================] - 51s 59ms/step - loss: 2.6237 - accuracy: 0.5324\n",
      "Epoch 49/50\n",
      "868/868 [==============================] - 50s 58ms/step - loss: 2.5132 - accuracy: 0.5444\n",
      "Epoch 50/50\n",
      "868/868 [==============================] - 48s 55ms/step - loss: 2.4811 - accuracy: 0.5471\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(tr_set, tr_label, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: does not consider adding Punctuation to the auto-generated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is it anyways erupt over honker weapons attack on healthcare bill about being\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"it is\"\n",
    "\n",
    "words_to_generate = 12\n",
    "for _ in range(words_to_generate):\n",
    "    line_seq = tokenizer.texts_to_sequences([seed_text])\n",
    "    padded = pad_sequences(line_seq, maxlen=max_seq_len-1, padding=\"pre\")\n",
    "    predict_probs = model.predict(padded,verbose=0)\n",
    "    predict_label = np.argmax(predict_probs,axis=1)\n",
    "    output_word = \"\"\n",
    "    for word,index in tokenizer.word_index.items(): # the index (tokenized \n",
    "        if predict_label == index:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction For Seen Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = pad_raw_data[:3,:]\n",
    "test_tr = test_sent[:,:-1]\n",
    "test_label = test_sent[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0, 3484,  235],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0, 3484,  235, 1485],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0, 3484,  235, 1485, 3485]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0, 3484],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0, 3484,  235],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0, 3484,  235, 1485]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 235, 1485, 3485])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 235, 1485, 3485], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pred = np.argmax(model.predict(test_tr,verbose=0),axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.027793778106570244,\n",
       " 0.02376152016222477,\n",
       " 0.030421946197748184,\n",
       " 0.038702476769685745,\n",
       " 0.049935195595026016,\n",
       " 0.06358006596565247,\n",
       " 0.08456940948963165,\n",
       " 0.11225518584251404,\n",
       " 0.14678139984607697,\n",
       " 0.18379175662994385,\n",
       " 0.21313363313674927,\n",
       " 0.23898328840732574,\n",
       " 0.2705933153629303,\n",
       " 0.28974655270576477,\n",
       " 0.2983871102333069,\n",
       " 0.32391273975372314,\n",
       " 0.3454781174659729,\n",
       " 0.3625071942806244,\n",
       " 0.37384793162345886,\n",
       " 0.38328051567077637,\n",
       " 0.3908050060272217,\n",
       " 0.40081363916397095,\n",
       " 0.4190308153629303,\n",
       " 0.4259432554244995,\n",
       " 0.42709532380104065,\n",
       " 0.43523186445236206,\n",
       " 0.45337700843811035,\n",
       " 0.46140551567077637,\n",
       " 0.4696860611438751,\n",
       " 0.476670503616333,\n",
       " 0.48635512590408325,\n",
       " 0.4921875,\n",
       " 0.5011880993843079,\n",
       " 0.5054003596305847,\n",
       " 0.5207013487815857,\n",
       " 0.5242295265197754,\n",
       " 0.5058683753013611,\n",
       " 0.5247696042060852,\n",
       " 0.5343822240829468,\n",
       " 0.5358942747116089,\n",
       " 0.5477750301361084,\n",
       " 0.5490711331367493,\n",
       " 0.5297738909721375,\n",
       " 0.31822437047958374,\n",
       " 0.4216949939727783,\n",
       " 0.48268288373947144,\n",
       " 0.5134648680686951,\n",
       " 0.5324380993843079,\n",
       " 0.5443548560142517,\n",
       " 0.547091007232666]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"accuracy\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
